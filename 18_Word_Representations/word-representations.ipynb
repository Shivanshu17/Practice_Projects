{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\n/kaggle/input/glove6b/glove.6B.100d.txt\n/kaggle/input/glove6b/glove.6B.200d.txt\n/kaggle/input/glove6b/glove.6B.300d.txt\n/kaggle/input/glove6b/glove.6B.50d.txt\n/kaggle/input/word2vec-google/GoogleNews-vectors-negative300.bin\n/kaggle/input/conceptnet-numberbatch-vectors/numberbatch-en.txt\n/kaggle/input/conceptnet-numberbatch-vectors/numberbatch-en-17.06.txt/numberbatch-en-17.06.txt\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing Data & Modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"from annoy import AnnoyIndex\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will import data when I am doing NER, POS, etc. tasks on it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will perform visualization after identifying the tasks and the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. CBOW Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Pretrained Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"class PreTrainedEmbeddings(object):\n    def __init__(self, word_to_index, word_vectors):\n        \"\"\"\n        Args:\n        word_to_index (dict): mapping from word to integers\n        word_vectors (list of numpy arrays)\n        \"\"\"\n        self.word_to_index = word_to_index\n        self.word_vectors = word_vectors\n        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n        self.index = AnnoyIndex(len(word_vectors[0]),metric='euclidean')\n        for _, i in self.word_to_index.items():\n            self.index.add_item(i, self.word_vectors[i])\n        self.index.build(50)\n        \n        \n    @classmethod\n    def from_embeddings_file(cls, embedding_file):\n        \"\"\"\n        Instantiate from pretrained vector file.\n        \n        Vector file should be of the format:\n            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\n            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\n        \n        Args:\n            embedding_file (str): location of the file\n        \n        Returns:\n            instance of PretrainedEmbeddings\n        \n        \"\"\"\n        word_to_index = {}\n        word_vectors = []\n        with open(embedding_file) as fp:\n            Initial_padding = 1\n            # positions_to_check = [2,4]\n            for position, line in enumerate(fp):\n                if position >= Initial_padding :\n                    #print(line)\n                    line = line.split(\" \")\n                    word = line[0]\n                    #print(word)\n                    #print(type(word))\n                    #print(float(line[3]))\n                    #print(type(float(line[3])))\n                    #print([x for x in line[1:-1]])\n                    vec = np.array([float(x) for x in line[1:-1]])\n                    word_to_index[word] = len(word_to_index)\n                    word_vectors.append(vec)\n                    #print(len(vec))\n             \n        return cls(word_to_index, word_vectors)\n    \n    def get_embedding(self, word):\n        \"\"\"\n        Args:\n        word (str)\n        Returns:\n            an embedding (numpy.ndarray)\n        \"\"\"\n        return self.word_vectors[self.word_to_index[word]]\n    \n    \n    def get_closest_to_vector(self, vector, n=1):\n        \"\"\"\n        Given a vector, return its n nearest neighbors\n\n        Args:\n            vector (np.ndarray): should match the size of the vectors\n            in the Annoy index\n            n (int): the number of neighbors to return\n        Returns:\n            [str, str, ...]: words nearest to the given vector\n            The words are not ordered by distance\n        \"\"\"\n        nn_indices = self.index.get_nns_by_vector(vector, n)\n        return [self.index_to_word[neighbor] for neighbor in nn_indices]\n    \n    \n    def compute_and_print_analogy(self, word1, word2, word3):\n        \"\"\"Prints the solutions to analogies using word embeddings\n        Analogies are word1 is to word2 as word3 is to __\n        This method will print: word1 : word2 :: word3 : word4\n        Args:\n            word1 (str)\n            word2 (str)\n            word3 (str)\n        \"\"\"\n        vec1 = self.get_embedding(word1)\n        vec2 = self.get_embedding(word2)\n        vec3 = self.get_embedding(word3)\n        # Simple hypothesis: Analogy is a spatial relationship\n        spatial_relationship = np.dot(vec2, vec1)\n        vec4 = vec3 + spatial_relationship\n        closest_words = self.get_closest_to_vector(vec4, n=4)\n        existing_words = set([word1, word2, word3])\n        closest_words = [word for word in closest_words if word not in existing_words]\n        if len(closest_words) == 0:\n            print(\"Could not find nearest neighbors for the vector!\")\n            return\n        \n        for word4 in closest_words:\n            print(\"{} : {} :: {} : {}\".format(word1, word2, word3,word4))\n            \n    ","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Have to remove the '\\n' in this data file and an initial padding of 1\nfasttext_embeddings_300d = PreTrainedEmbeddings.from_embeddings_file('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec')","execution_count":23,"outputs":[{"output_type":"stream","text":"300\n300\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't have to remove the '\\n' in this data file and no initial padding\nglove_embeddings_100d = PreTrainedEmbeddings.from_embeddings_file('../input/glove6b/glove.6B.100d.txt')","execution_count":28,"outputs":[{"output_type":"stream","text":"99\n99\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't have to remove the '\\n' in this data file and no initial padding\nglove_embeddings_200d = PreTrainedEmbeddings.from_embeddings_file('../input/glove6b/glove.6B.200d.txt')","execution_count":29,"outputs":[{"output_type":"stream","text":"199\n199\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't have to remove the '\\n' in this data file and no initial padding\nglove_embeddings_300d = PreTrainedEmbeddings.from_embeddings_file('../input/glove6b/glove.6B.300d.txt')","execution_count":30,"outputs":[{"output_type":"stream","text":"299\n299\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Have to figure out how to work on bin format\n# word2vec_embeddings_300d = PreTrainedEmbeddings.from_embeddings_file('../input/word2vec-google/GoogleNews-vectors-negative300.bin')","execution_count":31,"outputs":[{"output_type":"error","ename":"UnicodeDecodeError","evalue":"'utf-8' codec can't decode byte 0x94 in position 19: invalid start byte","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-bdbd1025394e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2vec_embeddings_300d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTrainedEmbeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_embeddings_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/word2vec-google/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-27-46103c7e95f6>\u001b[0m in \u001b[0;36mfrom_embeddings_file\u001b[0;34m(cls, embedding_file)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mInitial_padding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mpositions_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositions_to_check\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0;31m#print(line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x94 in position 19: invalid start byte"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't have to remove the '\\n' in this data file and an initial padding of 1\nnumberbatch_embeddings_300d = PreTrainedEmbeddings.from_embeddings_file('../input/conceptnet-numberbatch-vectors/numberbatch-en.txt')","execution_count":26,"outputs":[{"output_type":"stream","text":"300\n300\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 5. Application Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### A. Word Similarity Task"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### B.Word Analogy Task"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### C. NER task"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}