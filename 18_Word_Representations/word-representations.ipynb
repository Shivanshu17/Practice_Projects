{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/word2vec-google/GoogleNews-vectors-negative300.bin\n/kaggle/input/conceptnet-numberbatch-vectors/numberbatch-en.txt\n/kaggle/input/conceptnet-numberbatch-vectors/numberbatch-en-17.06.txt/numberbatch-en-17.06.txt\n/kaggle/input/glove6b/glove.6B.50d.txt\n/kaggle/input/glove6b/glove.6B.100d.txt\n/kaggle/input/glove6b/glove.6B.200d.txt\n/kaggle/input/glove6b/glove.6B.300d.txt\n/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing Data & Modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"from annoy import AnnoyIndex\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will import data when I am doing NER, POS, etc. tasks on it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will perform visualization after identifying the tasks and the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. CBOW Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Pretrained Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"class PreTrainedEmbeddings(object):\n    def __init__(self, word_to_index, word_vectors):\n        \"\"\"\n        Args:\n        word_to_index (dict): mapping from word to integers\n        word_vectors (list of numpy arrays)\n        \"\"\"\n        self.word_to_index = word_to_index\n        self.word_vectors = word_vectors\n        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n        self.index = AnnoyIndex(len(word_vectors[1]),metric='euclidean')\n        for _, i in self.word_to_index.items():\n            self.index.add_item(i, self.word_vectors[i])\n        self.index.build(50)\n        \n        \n    @classmethod\n    def from_embeddings_file(cls, embedding_file):\n        \"\"\"\n        Instantiate from pretrained vector file.\n        \n        Vector file should be of the format:\n            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\n            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\n        \n        Args:\n            embedding_file (str): location of the file\n        \n        Returns:\n            instance of PretrainedEmbeddings\n        \n        \"\"\"\n        word_to_index = {}\n        word_vectors = []\n        with open(embedding_file) as fp:\n            Initial_padding = 1\n            # positions_to_check = [2,4]\n            '''\n            for position, line in enumerate(fp):\n                if position >= Initial_padding :\n                    #print(line)\n                    line = line.split(\" \")\n                    word = line[0]\n                    #print(word)\n                    #print(type(word))\n                    #print(float(line[3]))\n                    #print(type(float(line[3])))\n                    #print([x for x in line[1:-1]])\n                    vec = np.array([float(x) for x in line[1:-1]])\n                    word_to_index[word] = len(word_to_index)\n                    word_vectors.append(vec)\n                    #print(len(vec))\n            '''\n                    \n            for line in fp.readlines():\n                line = line.split(\" \")\n                word = line[0]\n                vec = np.array([float(x) for x in line[1:-1]])\n                word_to_index[word] = len(word_to_index)\n                word_vectors.append(vec)\n             \n        return cls(word_to_index, word_vectors)\n    \n    def get_embedding(self, word):\n        \"\"\"\n        Args:\n        word (str)\n        Returns:\n            an embedding (numpy.ndarray)\n        \"\"\"\n        return self.word_vectors[self.word_to_index[word]]\n    \n    \n    def get_closest_to_vector(self, vector, n=1):\n        \"\"\"\n        Given a vector, return its n nearest neighbors\n\n        Args:\n            vector (np.ndarray): should match the size of the vectors\n            in the Annoy index\n            n (int): the number of neighbors to return\n        Returns:\n            [str, str, ...]: words nearest to the given vector\n            The words are not ordered by distance\n        \"\"\"\n        nn_indices = self.index.get_nns_by_vector(vector, n)\n        return [self.index_to_word[neighbor] for neighbor in nn_indices]\n    \n    \n    def print_closest_to_word(self, word, n = 1):\n        \"\"\"\n        Given a word, returns the closest neighbouring word\n        \n        Args:\n            word (str)\n            \n        Returns:\n            the nearest word (str)\n        \n        \"\"\"\n        vector_of_word = self.get_embedding(word)\n        closest_vectors = self.get_closest_to_vector(vector_of_word, n)\n        existing_words = set([word])\n        closest_words = [new_word for new_word in closest_vectors if new_word not in existing_words]\n        return closest_words\n    \n    \n    def compute_and_print_analogy(self, word1, word2, word3):\n        \"\"\"Prints the solutions to analogies using word embeddings\n        Analogies are word1 is to word2 as word3 is to __\n        This method will print: word1 : word2 :: word3 : word4\n        Args:\n            word1 (str)\n            word2 (str)\n            word3 (str)\n        \"\"\"\n        vec1 = self.get_embedding(word1)\n        vec2 = self.get_embedding(word2)\n        vec3 = self.get_embedding(word3)\n        # Simple hypothesis: Analogy is a spatial relationship\n        spatial_relationship = np.dot(vec2, vec1)\n        vec4 = vec3 + spatial_relationship\n        closest_words = self.get_closest_to_vector(vec4, n=4)\n        existing_words = set([word1, word2, word3])\n        closest_words = [word for word in closest_words if word not in existing_words]\n        if len(closest_words) == 0:\n            print(\"Could not find nearest neighbors for the vector!\")\n            return\n        \n        for word4 in closest_words:\n            print(\"{} : {} :: {} : {}\".format(word1, word2, word3,word4))\n            \n    ","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Have to remove the '\\n' in this data file and an initial padding of 1\nfasttext_embeddings_300d = PreTrainedEmbeddings.from_embeddings_file('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't have to remove the '\\n' in this data file and no initial padding\nglove_embeddings_100d = PreTrainedEmbeddings.from_embeddings_file('../input/glove6b/glove.6B.100d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't have to remove the '\\n' in this data file and no initial padding\nglove_embeddings_200d = PreTrainedEmbeddings.from_embeddings_file('../input/glove6b/glove.6B.200d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't have to remove the '\\n' in this data file and no initial padding\nglove_embeddings_300d = PreTrainedEmbeddings.from_embeddings_file('../input/glove6b/glove.6B.300d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Have to figure out how to work on bin format\n# word2vec_embeddings_300d = PreTrainedEmbeddings.from_embeddings_file('../input/word2vec-google/GoogleNews-vectors-negative300.bin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't have to remove the '\\n' in this data file and an initial padding of 1\nnumberbatch_embeddings_300d = PreTrainedEmbeddings.from_embeddings_file('../input/conceptnet-numberbatch-vectors/numberbatch-en.txt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Application Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### A. Word Similarity Task"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print_closest_to_word\nwordset_1 = fasttext_embeddings_300d.print_closest_to_word(\"cat\")\nwordset_2 = fasttext_embeddings_300d.print_closest_to_word(\"dinosarous\")\nwordset_3 = fasttext_embeddings_300d.print_closest_to_word(\"bulldog\", n= 4)\nwordset_4 = fasttext_embeddings_300d.print_closest_to_word(\"thoroughbred\", n=6)\nprint(wordset_3)\nprint(wordset_4)\n","execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'existing_words' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-702f42604ec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print_closest_to_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwordset_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext_embeddings_300d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_closest_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwordset_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext_embeddings_300d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_closest_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dinosarous\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwordset_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext_embeddings_300d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_closest_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bulldog\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwordset_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext_embeddings_300d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_closest_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"thoroughbred\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-4edfdfc4cedb>\u001b[0m in \u001b[0;36mprint_closest_to_word\u001b[0;34m(self, word, n)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mclosest_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_closest_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_of_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mexisting_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mclosest_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_word\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnew_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclosest_vectors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnew_word\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclosest_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-4edfdfc4cedb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mclosest_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_closest_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_of_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mexisting_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mclosest_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_word\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnew_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclosest_vectors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnew_word\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclosest_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'existing_words' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordset_1 = glove_embeddings_100d.print_closest_to_word(\"cat\")\nwordset_2 = glove_embeddings_100d.print_closest_to_word(\"dinosarous\")\nwordset_3 = glove_embeddings_100d.print_closest_to_word(\"bulldog\", n= 4)\nwordset_4 = glove_embeddings_100d.print_closest_to_word(\"thoroughbred\", n=6)\nprint(wordset_3)\nprint(wordset_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordset_1 = glove_embeddings_200d.print_closest_to_word(\"cat\")\nwordset_2 = glove_embeddings_200d.print_closest_to_word(\"dinosarous\")\nwordset_3 = glove_embeddings_200d.print_closest_to_word(\"bulldog\", n= 4)\nwordset_4 = glove_embeddings_200d.print_closest_to_word(\"thoroughbred\", n=6)\nprint(wordset_3)\nprint(wordset_4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordset_1 = glove_embeddings_300d.print_closest_to_word(\"cat\")\nwordset_2 = glove_embeddings_300d.print_closest_to_word(\"dinosarous\")\nwordset_3 = glove_embeddings_300d.print_closest_to_word(\"bulldog\", n= 4)\nwordset_4 = glove_embeddings_300d.print_closest_to_word(\"thoroughbred\", n=6)\nprint(wordset_3)\nprint(wordset_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordset_1 = numberbatch_embeddings_300d.print_closest_to_word(\"cat\")\nwordset_2 = numberbatch_embeddings_300d.print_closest_to_word(\"dinosarous\")\nwordset_3 = numberbatch_embeddings_300d.print_closest_to_word(\"bulldog\", n= 4)\nwordset_4 = numberbatch_embeddings_300d.print_closest_to_word(\"thoroughbred\", n=6)\nprint(wordset_3)\nprint(wordset_4)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### B.Word Analogy Task"},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute_and_print_analogy\nfasttext_embeddings_300d.compute_and_print_analogy(\"king\", \"man\", \"Queen\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_embeddings_100d.compute_and_print_analogy(\"king\", \"man\", \"Queen\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_embeddings_200d.compute_and_print_analogy(\"king\", \"man\", \"Queen\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_embeddings_300d.compute_and_print_analogy(\"king\", \"man\", \"Queen\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numberbatch_embeddings_300d.compute_and_print_analogy(\"king\", \"man\", \"Queen\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next up I'll create a function to formally test the performance of each model on word analogy datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### C. NER task"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will complete this later","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}